{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.python.platform import build_info as tf_build_info\n",
    "# print(tf_build_info.cuda_version_number)\n",
    "# # 9.0 in v1.10.0\n",
    "# print(tf_build_info.cudnn_version_number)\n",
    "# # 7 in v1.10.0\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "# import Image\n",
    "\n",
    "# if tf.__version__ < '1.4.0':\n",
    "#     raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  \n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# opener = urllib.request.URLopener()\n",
    "# print(1)\n",
    "# opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "# print(1)\n",
    "# MODEL_NAME = 'faster_rcnn_resnet101_kitti_2017_11_08'\n",
    "# MODEL_NAME = 'faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid_2017_11_08'\n",
    "# MODEL_NAME = 'ssd_inception_v2_coco_2017_11_17 (1)'\n",
    "# MODEL_NAME = 'facessd_mobilenet_v2_quantized_320x320_open_image_v4'\n",
    "# MODEL_NAME = 'faster_rcnn_resnet101_ava_v2.1_2018_04_30'\n",
    "# MODEL_NAME = 'faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid_2018_01_28'\n",
    "# MODEL_NAME = 'inception_v4_2016_09_09'\n",
    "# MODEL_NAME = 'mobilenet_v1_0.25_128_quant'\n",
    "MODEL_NAME = 'mask_rcnn_resnet50_atrous_coco_2018_01_28'\n",
    "\n",
    "# MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "# MODEL_NAME = 'rfcn_resnet101_coco_2017_11_08'\n",
    "\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "# PATH_TO_CKPT = 'inception_v4.ckpt'\n",
    "# PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map_modified_FOOD.pbtxt')\n",
    "NUM_CLASSES = 90\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "print(1)\n",
    "for file in tar_file.getmembers():\n",
    "    file_name = os.path.basename(file.name)\n",
    "    if 'frozen_inference_graph.pb' in file_name:\n",
    "#     if 'inception_v4.ckpt' in file_name:\n",
    "        tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "namelist=[]\n",
    "indexlist=[]\n",
    "for i in category_index.values():\n",
    "    namelist.append(i['name'])\n",
    "    indexlist.append(i['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import _thread\n",
    "import wave\n",
    "import struct\n",
    "\n",
    "def playSound(name):\n",
    "    import simpleaudio as sa\n",
    "\n",
    "    wave_obj = sa.WaveObject.from_wave_file('./Sounds/'+name)\n",
    "    play_obj = wave_obj.play()\n",
    "\n",
    "\n",
    "#     ####CRASHES ON FAST INPUT####\n",
    "#     import pyglet\n",
    "#     player = pyglet.media.Player()\n",
    "#     src = pyglet.media.load(name)\n",
    "#     player.volume = 0.1\n",
    "#     player.queue(src)\n",
    "#     player.play()\n",
    "\n",
    "#     ####VERY SLOW####\n",
    "#     import pygame.mixer\n",
    "#     pm = pygame.mixer\n",
    "#     pm.init()\n",
    "#     sound = pm.Sound(name)\n",
    "#     sound.set_volume(0.5)\n",
    "#     sound.play()\n",
    "\n",
    "\n",
    "\n",
    "def drawEllipse(contours, text):\n",
    "    if(contours == None or len(contours) == 0):\n",
    "        return ((-100,-100), None)\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "    if(cv2.contourArea(c) < 500):\n",
    "        return ((-100,-100), None)\n",
    "    ellipse = cv2.fitEllipse(c)\n",
    "    cv2.ellipse(image_np, ellipse, (0,0,0), 2)\n",
    "\n",
    "    blank = np.zeros(image_np.shape[0:2])\n",
    "    ellipseImage = cv2.ellipse(blank, ellipse, (255, 255, 255), -2)\n",
    "    # cv2.imshow(\"ell\",ellipseImage)\n",
    "\n",
    "    M = cv2.moments(c)\n",
    "    if M[\"m00\"] == 0:\n",
    "        return\n",
    "    center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    "\n",
    "    if radius > 10:\n",
    "        # draw the ellipse and centroid on the frame,\n",
    "        # then update the list of tracked points\n",
    "        # cv2.circle(img, (int(x), int(y)), int(radius),(0, 0, 0), 2)\n",
    "        cv2.circle(image_np, center, 3, (0, 0, 255), -1)\n",
    "        cv2.putText(image_np,text, (center[0]+10,center[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.4,(0, 0, 0),2)\n",
    "        cv2.putText(image_np,\"(\"+str(center[0])+\",\"+str(center[1])+\")\", (center[0]+10,center[1]+15), cv2.FONT_HERSHEY_SIMPLEX, 0.4,(0, 0, 0),1)\n",
    "\n",
    "    return (center, ellipseImage)\n",
    "\n",
    "def detectCollision(imgA, imgB, velocity, touching, name):\n",
    "    mA = cv2.moments(imgA, False)\n",
    "    mB = cv2.moments(imgB, False)\n",
    "    blank = np.zeros(image_np.shape[0:2])\n",
    "    if type(imgA) == type(None) or type(imgB) == type(None):\n",
    "        return\n",
    "    intersection = cv2.bitwise_and(imgA, imgB)\n",
    "    area = cv2.countNonZero(intersection)\n",
    "    if area < 20:\n",
    "        touching = False\n",
    "    if area > 100 and not touching:\n",
    "        # print(int(mA[\"m01\"] / mA[\"m00\"])< int(mB[\"m01\"] / mB[\"m00\"]))\n",
    "        # print(area)\n",
    "        if int(mA[\"m01\"] / mA[\"m00\"])< int(mB[\"m01\"] / mB[\"m00\"]):\n",
    "            if velocity > 0: # default is 10; this is the score that decides whether snare will be hit or not\n",
    "                _thread.start_new_thread(playSound, (name,))\n",
    "                # playSound(name)\n",
    "        touching = True\n",
    "    return touching\n",
    "\n",
    "def newDrum(pos, name):\n",
    "    # pos = (x, y)\n",
    "    drum = cv2.circle(image_np,pos, 50,(0,0, 0),5)\n",
    "    cv2.putText(drum,name,pos,cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "    blank = np.zeros(image_np.shape[0:2])\n",
    "    drum_image = cv2.circle(blank.copy(), pos, 50, (255, 255, 255), -5)\n",
    "    global numDrums\n",
    "    numDrums += 1\n",
    "    return (name, drum_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# music note repository\n",
    "notes = [\n",
    "'A3vH',\n",
    "'A4vH',\n",
    "'A5vH',\n",
    "'A6vH',\n",
    "'A7vH',\n",
    "'B1vH',\n",
    "'B2vH',\n",
    "'B7vH',\n",
    "'C1vH',\n",
    "'C4vH',\n",
    "'C5vH',\n",
    "'C6vH',\n",
    "'C7vH',\n",
    "'D#2vH',\n",
    "'D#3vH',\n",
    "'D#4vH',\n",
    "'D#5vH',\n",
    "'D#6vH',\n",
    "'D#7vH',\n",
    "'F#1vH',\n",
    "'F#2vH',\n",
    "'F#3vH',\n",
    "'F#4vH',\n",
    "'F#5vH',\n",
    "'F#6vH',\n",
    "'F#7vH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A#1.wav',\n",
       " 'A#3.wav',\n",
       " 'A#4.wav',\n",
       " 'A#5.wav',\n",
       " 'A1.wav',\n",
       " 'A2.wav',\n",
       " 'A3.wav',\n",
       " 'A4.wav',\n",
       " 'A5.wav',\n",
       " 'B1.wav',\n",
       " 'B2.wav',\n",
       " 'B3.wav',\n",
       " 'B4.wav',\n",
       " 'B5.wav',\n",
       " 'C#2.wav',\n",
       " 'C#4.wav',\n",
       " 'C#5.wav',\n",
       " 'C2.wav',\n",
       " 'C3.wav',\n",
       " 'C4.wav',\n",
       " 'C5.wav',\n",
       " 'C6.wav',\n",
       " 'D#2.wav',\n",
       " 'D#4.wav',\n",
       " 'D#5.wav',\n",
       " 'D2.wav',\n",
       " 'D3.wav',\n",
       " 'D4.wav',\n",
       " 'D5.wav',\n",
       " 'E2.wav',\n",
       " 'E3.wav',\n",
       " 'E4.wav',\n",
       " 'E5.wav',\n",
       " 'F#3.wav',\n",
       " 'F#4.wav',\n",
       " 'F#5.wav',\n",
       " 'F2.wav',\n",
       " 'F3.wav',\n",
       " 'F4.wav',\n",
       " 'F5.wav',\n",
       " 'G#1.wav',\n",
       " 'G#3.wav',\n",
       " 'G#5.wav',\n",
       " 'G1.wav',\n",
       " 'G2.wav',\n",
       " 'G3.wav',\n",
       " 'G4.wav',\n",
       " 'G5.wav',\n",
       " 'readme.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = './Guitar sounds'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Song Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import cv2\n",
    "# cap=cv2.VideoCapture(1) # 0 stands for very first webcam attach\n",
    "# # https://www.youtube.com/watch?v=BUrR6BTx6Mk\n",
    "# filename=\"outputtest.avi\"#[place were i stored my output file]\n",
    "# codec=cv2.VideoWriter_fourcc('m','p','4','v')#fourcc stands for four character code\n",
    "# framerate=30\n",
    "# resolution=(640,480)\n",
    "# # resolution=(1080,640)\n",
    "\n",
    "# frameCount = 0\n",
    "# timeStart = time.time()\n",
    "\n",
    "# b1 = (0,0)\n",
    "# b2 = (0,0)\n",
    "# currentBlueVelocity = 0\n",
    "# r1 = (0,0)\n",
    "# r2 = (0,0)\n",
    "# currentRedVelocity = 0\n",
    "\n",
    "# blueAndSnare = False\n",
    "# blueAndHiHat = False\n",
    "# redAndSnare = False\n",
    "# redAndHiHat = False\n",
    "# # booli  = [False for i in range(2)]\n",
    "# booli  = [False for i in range(1)]\n",
    "\n",
    "# numDrums = 0\n",
    "# # drums = [None for i in range(2)]\n",
    "# drums = [None for i in range(1)]\n",
    "\n",
    "# item_lost = True\n",
    "# # item_lost = False\n",
    "# counter=0   \n",
    "# tmarker=0\n",
    "# VideoFileOutput=cv2.VideoWriter(filename,codec,framerate, resolution)\n",
    "    \n",
    "# with detection_graph.as_default():\n",
    "#     with tf.Session(graph=detection_graph) as sess:\n",
    "#         ret=True\n",
    "#         while (ret):\n",
    "            \n",
    "#             ret, image_np=cap.read() \n",
    "\n",
    "#             f = open('output.txt', 'r')\n",
    "#             x = [x.replace(\"\\n\", \"\") for x in f.readlines()][-1]\n",
    "#             for i in range(len(namelist)):\n",
    "#                 if namelist[i] in x:\n",
    "#     #                 print(content[i], i)\n",
    "#                     tmarker=indexlist[i]\n",
    "#     #         print(tmarker)\n",
    "\n",
    "#             if tmarker != -1:\n",
    "#                 item_lost = True\n",
    "#             if \"found\" in x:\n",
    "#                 tmarker=-1\n",
    "                \n",
    "                \n",
    "#             now = time.time()\n",
    "#             fps = frameCount / (now - timeStart+1.0)\n",
    "#             frameCount += 1\n",
    "\n",
    "# #             _, img = cap.read()\n",
    "# #             image_np = cv2.flip(image_np, 1)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "#             # Definite input and output Tensors for detection_graph\n",
    "#             image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "#             # Each box represents a part of the image where a particular object was detected.\n",
    "#             detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "#             # Each score represent how level of confidence for each of the objects.\n",
    "#             # Score is shown on the result image, together with the class label.\n",
    "#             detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "#             detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "#             num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "#               # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "#             image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "#               # Actual detection.\n",
    "#             (boxes, scores, classes, num) = sess.run(\n",
    "#                   [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "#                   feed_dict={image_tensor: image_np_expanded})\n",
    "#               # Visualization of the results of a detection.\n",
    "#             if item_lost:\n",
    "#                 image_np = vis_util.lost_item_mode(image_np,\n",
    "#                   np.squeeze(boxes),\n",
    "#                   np.squeeze(classes).astype(np.int32),\n",
    "#                   np.squeeze(scores),\n",
    "#                   category_index,\n",
    "#                   use_normalized_coordinates=True,\n",
    "#                   line_thickness=8,\n",
    "#                   marker=tmarker)\n",
    "# #                 cv2.imwrite(r'C:/Users/user/Desktop/Moverio-MemoryPalace-Instructions/Calhacks-5.0/Object-Detection-On-Live-Video-Feed/models/object_detection/data_images/img_'+str(counter)+'.jpg', image_np)\n",
    "#                 counter+=1\n",
    "#             else:\n",
    "#                 image_np = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "#                   image_np,\n",
    "#                   np.squeeze(boxes),\n",
    "#                   np.squeeze(classes).astype(np.int32),\n",
    "#                   np.squeeze(scores),\n",
    "#                   category_index,\n",
    "#                   use_normalized_coordinates=True,\n",
    "#                   line_thickness=8)\n",
    "# #                 cv2.imwrite(r'C:/Users/user/Desktop/Moverio-MemoryPalace-Instructions/Calhacks-5.0/Object-Detection-On-Live-Video-Feed/models/object_detection/data_images/img_'+str(counter)+'.jpg', image_np)\n",
    "#                 counter+=1\n",
    "    \n",
    "#             # cv2.putText(img,\"FPS : \",(10,100),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)\n",
    "#             cv2.putText(image_np,\"FPS: %.2f\" % (fps),(10,100),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)\n",
    "\n",
    "#             # Add the drums\n",
    "# #             drums[0] = newDrum((350, 400), \"snare\")\n",
    "# #             drums[1] = newDrum((100, 400), \"hi_hat\")\n",
    "# #             drums[0] = newDrum((800, 400), \"snare (stick 1)\")\n",
    "# #             drums[1] = newDrum((300, 400), \"hi_hat (stick 2)\")\n",
    "#             rand1 = random.randint(0,len(notes)-1)\n",
    "#             rand2 = random.randint(0,len(notes)-1)\n",
    "#             drums[0] = newDrum((400, 400), notes[rand1])\n",
    "# #             drums[1] = newDrum((100, 400), notes[rand2])\n",
    "\n",
    "#             #converting frame(img i.e BGR) to HSV (hue-saturation-value)\n",
    "#             hsv=cv2.cvtColor(image_np,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "#             #defining the range of red color\n",
    "# #             red_lower=np.array([255,255,255],np.uint8)\n",
    "# #             red_upper=np.array([255,255,255],np.uint8)\n",
    "#             red_lower=np.array([0,0,255],np.uint8)\n",
    "#             red_upper=np.array([255,255,255],np.uint8)\n",
    "\n",
    "#             #defining the Range of Blue color\n",
    "#             blue_lower=np.array([0,0,255],np.uint8)\n",
    "#             blue_upper=np.array([255,255,255],np.uint8)\n",
    "\n",
    "#             #finding the range of red,blue color in the image\n",
    "#             red=cv2.inRange(hsv, red_lower, red_upper)\n",
    "#             blue=cv2.inRange(hsv,blue_lower,blue_upper)\n",
    "\n",
    "#             #Morphological transformation, Dilation\n",
    "#             kernal = np.ones((5 ,5), \"uint8\")\n",
    "\n",
    "#             red=cv2.dilate(red, kernal)\n",
    "#             res=cv2.bitwise_and(image_np, image_np, mask = red)\n",
    "\n",
    "#             blue=cv2.dilate(blue,kernal)\n",
    "#             res1=cv2.bitwise_and(image_np, image_np, mask = blue)\n",
    "\n",
    "\n",
    "#             #Tracking the Red Color\n",
    "#             (contours,hierarchy)=cv2.findContours(red,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "#             (redCenter, redEllipse) = drawEllipse(contours, \"Red\")\n",
    "# #             cv2.drawContours(image_np, contours, -1 , (0,0,255), 2)\n",
    "\n",
    "\n",
    "#             #Tracking the Blue Color\n",
    "#             (contours,hierarchy)=cv2.findContours(blue,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "# #             cv2.drawContours(image_np, contours, -1 , (255,0,0), 2)\n",
    "#             (blueCenter, blueEllipse) = drawEllipse(contours, \"Blue\")\n",
    "\n",
    "#             b1 = b2\n",
    "#             b2 = blueCenter\n",
    "#             bDelta = math.sqrt((b2[0] - b1[0])**2 + (b2[1] - b1[1])**2)\n",
    "#             bVelocity = bDelta * fps / 100\n",
    "#             if (bVelocity - currentBlueVelocity) > 10:\n",
    "#                 cv2.putText(image_np,str(int(bVelocity)),(10, 50),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "#             else:\n",
    "#                 cv2.putText(image_np,str(int(currentBlueVelocity)),(10, 50),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "#             currentBlueVelocity = bVelocity\n",
    "\n",
    "#             r1 = r2\n",
    "#             r2 = redCenter\n",
    "#             rDelta = math.sqrt((r2[0] - r1[0])**2 + (r2[1] - r1[1])**2)\n",
    "#             rVelocity = rDelta * fps / 100\n",
    "#             if (rVelocity - currentRedVelocity) > 10:\n",
    "#                 cv2.putText(image_np,str(int(rVelocity)),(70, 50),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "#             else:\n",
    "#                 cv2.putText(image_np,str(int(currentRedVelocity)),(70, 50),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "#             currentRedVelocity = rVelocity\n",
    "\n",
    "\n",
    "#             for i in range(len(drums)):\n",
    "#                 # print(booli)\n",
    "#                 booli[i] = detectCollision(blueEllipse, drums[i][1], currentBlueVelocity, booli[i], \"{0}.wav\".format(drums[i][0]))\n",
    "#             ### comment out\n",
    "# #             blueAndSnare = detectCollision(blueEllipse, drums[0][1], blueAndSnare, \"snare.wav\")\n",
    "# #             blueAndHiHat = detectCollision(blueEllipse, drums[1][1], blueAndHiHat, \"hi_hat.wav\")\n",
    "\n",
    "# #             blueAndSnare = detectCollision(blueEllipse, snare_image, blueAndSnare, \"snare.wav\")\n",
    "# #             blueAndHiHat = detectCollision(blueEllipse, hi_hat_image, blueAndHiHat, \"Closed-Hi-Hat.wav\")\n",
    "            \n",
    "# #             redAndSnare = detectCollision(redEllipse, snare_image, redAndSnare, \"snare.wav\")\n",
    "# #             redAndHiHat = detectCollision(redEllipse, hi_hat_image, redAndHiHat, \"Closed-Hi-Hat.wav\")\n",
    "#             ###\n",
    "\n",
    "\n",
    "#             #cv2.imshow(\"Redcolour\",red)\n",
    "# #             cv2.imshow(\"Color Tracking\",img)\n",
    "# #             #cv2.imshow(\"red\",res)\n",
    "# #             if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "# #                 cap.release()\n",
    "# #                 cv2.destroyAllWindows()\n",
    "# #                 break    \n",
    "\n",
    "#             VideoFileOutput.write(image_np)\n",
    "#             cv2.imshow(\"Redcolour\",red)\n",
    "#             cv2.imshow('live_detection',image_np)\n",
    "#             cv2.imshow(\"red\",res)\n",
    "#             if cv2.waitKey(25) & 0xFF==ord('q'):\n",
    "#                 break\n",
    "#                 cv2.destroyAllWindows()\n",
    "#                 cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canon in D Major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "guitarChords = [\n",
    "['C2','E3', 'G3', 'C3', 'E4'],\n",
    "['F2','A2','D2','A3','D3'],\n",
    "['E2','B2','E3','G3','B3'],\n",
    "['F2','A2','E3','G3','B3'],    \n",
    "['G2','B2','D2','G3','B3'],    \n",
    "['A2','E3','A3','C3','E4'],\n",
    "['G2','B2','D2','G3','B3'],\n",
    "['C2','E3','G3','C3','E4']\n",
    "]\n",
    "\n",
    "# ｃ2　ｅ3　ｇ3　ｃ3　ｅ4\n",
    "# ｇ2　ｂ2　ｄ2　ｇ3　ｂ3　\n",
    "# ａ2　ｅ3　ａ3　ｃ3　ｅ4\n",
    "# ｅ2　ｂ2　ｅ3　ｇ3　ｂ3　　\n",
    "# ｆ2　ａ2　ｄ2　ａ3　ｄ3　\n",
    "# ｃ2　ｅ3　ｇ3　ｃ3　ｅ4\n",
    "# ｆ2　ａ2　ｆ3　ａ3　ｃ3　\n",
    "# ｇ2　ｂ2　ｄ2　ｇ3　ｂ3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.0.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\shapedescr.cpp:310: error: (-201:Incorrect size of input array) There should be at least 5 points to fit the ellipse in function 'cv::fitEllipse'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c6a6e0156fad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;31m#Tracking the Red Color\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mcontours\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhierarchy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETR_TREE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCHAIN_APPROX_SIMPLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mredCenter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredEllipse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrawEllipse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontours\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Red\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;31m#             cv2.drawContours(image_np, contours, -1 , (0,0,255), 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-964c4b0740c6>\u001b[0m in \u001b[0;36mdrawEllipse\u001b[1;34m(contours, text)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontourArea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mellipse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitEllipse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mellipse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.0.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\shapedescr.cpp:310: error: (-201:Incorrect size of input array) There should be at least 5 points to fit the ellipse in function 'cv::fitEllipse'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "cap=cv2.VideoCapture(1) # 0 stands for very first webcam attach\n",
    "# https://www.youtube.com/watch?v=BUrR6BTx6Mk\n",
    "filename=\"outputtest.avi\"#[place were i stored my output file]\n",
    "codec=cv2.VideoWriter_fourcc('m','p','4','v')#fourcc stands for four character code\n",
    "framerate=30\n",
    "resolution=(640,480)\n",
    "# resolution=(1080,640)\n",
    "\n",
    "frameCount = 0\n",
    "timeStart = time.time()\n",
    "\n",
    "b1 = (0,0)\n",
    "b2 = (0,0)\n",
    "currentBlueVelocity = 0\n",
    "r1 = (0,0)\n",
    "r2 = (0,0)\n",
    "currentRedVelocity = 0\n",
    "\n",
    "blueAndSnare = False\n",
    "blueAndHiHat = False\n",
    "redAndSnare = False\n",
    "redAndHiHat = False\n",
    "# booli  = [False for i in range(2)]\n",
    "booli  = [False for i in range(2+2+len(guitarChords[0]))]\n",
    "\n",
    "numDrums = 0\n",
    "# drums = [None for i in range(2)]\n",
    "drums = [None for i in range(2+2+len(guitarChords[0]))]\n",
    "\n",
    "item_lost = True\n",
    "# item_lost = False\n",
    "counter=0   \n",
    "tmarker=0\n",
    "VideoFileOutput=cv2.VideoWriter(filename,codec,framerate, resolution)\n",
    "    \n",
    "chordCounter = 0\n",
    "    \n",
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        ret=True\n",
    "        while (ret):\n",
    "            \n",
    "            ret, image_np=cap.read() \n",
    "\n",
    "            f = open('output.txt', 'r')\n",
    "            x = [x.replace(\"\\n\", \"\") for x in f.readlines()][-1]\n",
    "            for i in range(len(namelist)):\n",
    "                if namelist[i] in x:\n",
    "    #                 print(content[i], i)\n",
    "                    tmarker=indexlist[i]\n",
    "    #         print(tmarker)\n",
    "\n",
    "            if tmarker != -1:\n",
    "                item_lost = True\n",
    "            if \"found\" in x:\n",
    "                tmarker=-1\n",
    "                \n",
    "                \n",
    "            now = time.time()\n",
    "            fps = frameCount / (now - timeStart+1.0)\n",
    "            frameCount += 1\n",
    "\n",
    "#             _, img = cap.read()\n",
    "#             image_np = cv2.flip(image_np, 1)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            # Definite input and output Tensors for detection_graph\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            # Each box represents a part of the image where a particular object was detected.\n",
    "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            # Each score represent how level of confidence for each of the objects.\n",
    "            # Score is shown on the result image, together with the class label.\n",
    "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "              # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "              # Actual detection.\n",
    "            (boxes, scores, classes, num) = sess.run(\n",
    "                  [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "                  feed_dict={image_tensor: image_np_expanded})\n",
    "              # Visualization of the results of a detection.\n",
    "            if item_lost:\n",
    "                image_np = vis_util.lost_item_mode(image_np,\n",
    "                  np.squeeze(boxes),\n",
    "                  np.squeeze(classes).astype(np.int32),\n",
    "                  np.squeeze(scores),\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  line_thickness=8,\n",
    "                  marker=tmarker)\n",
    "#                 cv2.imwrite(r'C:/Users/user/Desktop/Moverio-MemoryPalace-Instructions/Calhacks-5.0/Object-Detection-On-Live-Video-Feed/models/object_detection/data_images/img_'+str(counter)+'.jpg', image_np)\n",
    "                counter+=1\n",
    "            else:\n",
    "                image_np = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                  image_np,\n",
    "                  np.squeeze(boxes),\n",
    "                  np.squeeze(classes).astype(np.int32),\n",
    "                  np.squeeze(scores),\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  line_thickness=8)\n",
    "#                 cv2.imwrite(r'C:/Users/user/Desktop/Moverio-MemoryPalace-Instructions/Calhacks-5.0/Object-Detection-On-Live-Video-Feed/models/object_detection/data_images/img_'+str(counter)+'.jpg', image_np)\n",
    "                counter+=1\n",
    "    \n",
    "            # cv2.putText(img,\"FPS : \",(10,100),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)\n",
    "            cv2.putText(image_np,\"FPS: %.2f\" % (fps),(10,100),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 2)\n",
    "\n",
    "            # Add the drums\n",
    "            \n",
    "#             drums[0] = newDrum((800, 400), \"snare (stick 1)\")\n",
    "#             drums[1] = newDrum((300, 400), \"hi_hat (stick 2)\")\n",
    "\n",
    "            chords = guitarChords[chordCounter]\n",
    "            base = 350\n",
    "            drums[0] = newDrum((base, base), chords[0])\n",
    "            drums[1] = newDrum((base+25, base), chords[1])\n",
    "            drums[2] = newDrum((base+50, base), chords[2])\n",
    "            drums[3] = newDrum((base-25, base), chords[3])\n",
    "            drums[4] = newDrum((base-50, base), chords[4])\n",
    "            chordCounter += 1\n",
    "            if chordCounter == 5:\n",
    "                chordCounter=0\n",
    "                \n",
    "            drums[5] = newDrum((0, 50), \"snare\")\n",
    "            drums[6] = newDrum((0, 100), \"hi_hat\")\n",
    "\n",
    "            rand1 = random.randint(0,len(notes)-1)\n",
    "            rand2 = random.randint(0,len(notes)-1)\n",
    "            drums[7] = newDrum((0, 400), notes[rand1])\n",
    "            drums[8] = newDrum((0, 300), notes[rand2])\n",
    "\n",
    "            #converting frame(img i.e BGR) to HSV (hue-saturation-value)\n",
    "            hsv=cv2.cvtColor(image_np,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            #defining the range of red color\n",
    "#             red_lower=np.array([255,255,255],np.uint8)\n",
    "#             red_upper=np.array([255,255,255],np.uint8)\n",
    "            red_lower=np.array([0,0,255],np.uint8)\n",
    "            red_upper=np.array([255,255,255],np.uint8)\n",
    "\n",
    "            #defining the Range of Blue color\n",
    "            blue_lower=np.array([0,0,255],np.uint8)\n",
    "            blue_upper=np.array([255,255,255],np.uint8)\n",
    "\n",
    "            #finding the range of red,blue color in the image\n",
    "            red=cv2.inRange(hsv, red_lower, red_upper)\n",
    "            blue=cv2.inRange(hsv,blue_lower,blue_upper)\n",
    "\n",
    "            #Morphological transformation, Dilation\n",
    "            kernal = np.ones((5 ,5), \"uint8\")\n",
    "\n",
    "            red=cv2.dilate(red, kernal)\n",
    "            res=cv2.bitwise_and(image_np, image_np, mask = red)\n",
    "\n",
    "            blue=cv2.dilate(blue,kernal)\n",
    "            res1=cv2.bitwise_and(image_np, image_np, mask = blue)\n",
    "\n",
    "\n",
    "            #Tracking the Red Color\n",
    "            (contours,hierarchy)=cv2.findContours(red,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "            (redCenter, redEllipse) = drawEllipse(contours, \"Red\")\n",
    "#             cv2.drawContours(image_np, contours, -1 , (0,0,255), 2)\n",
    "\n",
    "\n",
    "            #Tracking the Blue Color\n",
    "            (contours,hierarchy)=cv2.findContours(blue,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "#             cv2.drawContours(image_np, contours, -1 , (255,0,0), 2)\n",
    "            (blueCenter, blueEllipse) = drawEllipse(contours, \"Blue\")\n",
    "\n",
    "            b1 = b2\n",
    "            b2 = blueCenter\n",
    "            bDelta = math.sqrt((b2[0] - b1[0])**2 + (b2[1] - b1[1])**2)\n",
    "            bVelocity = bDelta * fps / 100\n",
    "            if (bVelocity - currentBlueVelocity) > 10:\n",
    "                cv2.putText(image_np,str(int(bVelocity)),(10, 50),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "            else:\n",
    "                cv2.putText(image_np,str(int(currentBlueVelocity)),(10, 50),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "            currentBlueVelocity = bVelocity\n",
    "\n",
    "            r1 = r2\n",
    "            r2 = redCenter\n",
    "            rDelta = math.sqrt((r2[0] - r1[0])**2 + (r2[1] - r1[1])**2)\n",
    "            rVelocity = rDelta * fps / 100\n",
    "            if (rVelocity - currentRedVelocity) > 10:\n",
    "                cv2.putText(image_np,str(int(rVelocity)),(70, 50),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "            else:\n",
    "                cv2.putText(image_np,str(int(currentRedVelocity)),(70, 50),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "            currentRedVelocity = rVelocity\n",
    "\n",
    "\n",
    "            for i in range(len(drums)):\n",
    "                # print(booli)\n",
    "                booli[i] = detectCollision(blueEllipse, drums[i][1], currentBlueVelocity, booli[i], \"{0}.wav\".format(drums[i][0]))\n",
    "            ### comment out\n",
    "#             blueAndSnare = detectCollision(blueEllipse, drums[0][1], blueAndSnare, \"snare.wav\")\n",
    "#             blueAndHiHat = detectCollision(blueEllipse, drums[1][1], blueAndHiHat, \"hi_hat.wav\")\n",
    "\n",
    "#             blueAndSnare = detectCollision(blueEllipse, snare_image, blueAndSnare, \"snare.wav\")\n",
    "#             blueAndHiHat = detectCollision(blueEllipse, hi_hat_image, blueAndHiHat, \"Closed-Hi-Hat.wav\")\n",
    "            \n",
    "#             redAndSnare = detectCollision(redEllipse, snare_image, redAndSnare, \"snare.wav\")\n",
    "#             redAndHiHat = detectCollision(redEllipse, hi_hat_image, redAndHiHat, \"Closed-Hi-Hat.wav\")\n",
    "            ###\n",
    "\n",
    "\n",
    "            #cv2.imshow(\"Redcolour\",red)\n",
    "#             cv2.imshow(\"Color Tracking\",img)\n",
    "#             #cv2.imshow(\"red\",res)\n",
    "#             if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                 cap.release()\n",
    "#                 cv2.destroyAllWindows()\n",
    "#                 break    \n",
    "\n",
    "            VideoFileOutput.write(image_np)\n",
    "            cv2.imshow(\"Redcolour\",red)\n",
    "            cv2.imshow('live_detection',image_np)\n",
    "            cv2.imshow(\"red\",res)\n",
    "            if cv2.waitKey(25) & 0xFF==ord('q'):\n",
    "                break\n",
    "                cv2.destroyAllWindows()\n",
    "                cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sound source: http://freepats.zenvoid.org/Guitar/steel-acoustic-guitar.html\n",
    "# Canon in D Major demo: https://www.youtube.com/watch?v=Kvig-EYYM3w\n",
    "# chords (Canon in D Major)\n",
    "# ｃ2　ｅ3　ｇ3　ｃ3　ｅ4\n",
    "# ｇ2　ｂ2　ｄ2　ｇ3　ｂ3　\n",
    "# ａ2　ｅ3　ａ3　ｃ3　ｅ4\n",
    "# ｅ2　ｂ2　ｅ3　ｇ3　ｂ3　　\n",
    "# ｆ2　ａ2　ｄ2　ａ3　ｄ3　\n",
    "# ｃ2　ｅ3　ｇ3　ｃ3　ｅ4\n",
    "# ｆ2　ａ2　ｆ3　ａ3　ｃ3　\n",
    "# ｇ2　ｂ2　ｄ2　ｇ3　ｂ3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moverio",
   "language": "python",
   "name": "moverio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
