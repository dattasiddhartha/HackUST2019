### Music Generation through Food Detection, Assignment of Music Notes to Distinct Items, and Utensil Impact Detection

<img src="http://siddharthadatta.ml/images/armusic.png" height="300"/>

This is an augmented reality system/game where people can see highlights (bounding boxes) over their food, and tapping their food will produce a certain instrumental sound to synthesize music (including guitar chords, piano on different scales, and drums); it executed on Mixed Reality lens with object detection. Further details can be found [here](http://siddharthadatta.ml/portfolio/10004armusic/).

Model weights used can be found [here](https://drive.google.com/drive/folders/1tWczVaRlEEcPgvOBpk5LWGwZWvnaFuMk?usp=sharing) and shouuld be paced in the *CVMusicSynthesis* directory; else the complete repository can also be downloaded from [here](https://bit.ly/2P7YYRQ). Download the repository and run the *objectdetection+airdrums-ver2.ipynb* notebook to run the model. 

Cringey demo videos: [[Demo 1]](https://bit.ly/2GcKUCl) [[Demo 2]](https://bit.ly/2UY9FM0)

*|!| Apologies for the messy codebase, this was a quick'n'dirty submission for a hackathon "HackUST2019" (entitled "Dancing with the Pizza").*
